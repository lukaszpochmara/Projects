# Projects# PySpark ETL demo: CSV â†’ Parquet na S3/MinIO

## Opis projektu

Ten projekt demonstruje prosty pipeline ETL oparty o PySpark i MinIO (symulacjÄ™ AWS S3).  
Celem jest pobranie pliku CSV z â€žchmuryâ€ (MinIO), przetworzenie (czyszczenie i agregacja) w PySpark,  
a nastÄ™pnie zapisanie efektu do formatu Parquet w MinIO.

---

## Krok po kroku: Jak uruchomiÄ‡ lokalnie?

### 1. Uruchom MinIO (Docker Compose)
```bash
docker-compose up -d
2. StwÃ³rz bucket demo w MinIO (http://localhost:9001)

    StwÃ³rz foldery: input i output.

3. WrzuÄ‡ przykÅ‚adowy plik CSV do demo/input/ (np. sample.csv).
4. Odpal pipeline PySpark:

spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.367 src/etl_pipeline.py

5. Wynik pojawi siÄ™ jako plik Parquet w demo/output/.
Co zobaczysz w MinIO po uruchomieniu?

    demo/input/sample.csv â€” surowe dane wejÅ›ciowe (CSV)

    demo/output/demo.parquet â€” dane wyjÅ›ciowe w formacie Parquet (po przetworzeniu przez Spark)

Wymagania

    Docker + Docker Compose

    PySpark (>=3.3)

    Python 3.8+

    MinIO (uruchamiany przez Docker Compose)

    (Opcjonalnie) AWS CLI lub mc (MinIO Client)

PySpark ETL demo: CSV â†’ Parquet on S3/MinIO
Project description

This project demonstrates a simple ETL pipeline using PySpark and MinIO (S3-compatible storage).
The goal is to fetch a CSV file from the "cloud" (MinIO), process it in PySpark (cleaning and aggregation),
and save the result as a Parquet file back to MinIO.
Step by step: How to run locally?
1. Start MinIO (Docker Compose)

docker-compose up -d

2. Create a demo bucket in MinIO (http://localhost:9001)

    Create input and output folders.

3. Upload a sample CSV file to demo/input/ (e.g., sample.csv).
4. Run the PySpark pipeline:

spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.367 src/etl_pipeline.py

5. The result will appear as a Parquet file in demo/output/.
What will you see in MinIO after running the pipeline?

    demo/input/sample.csv â€” raw input data (CSV)

    demo/output/demo.parquet â€” processed data (Parquet, generated by Spark)

Requirements

    Docker + Docker Compose

    PySpark (>=3.3)

    Python 3.8+

    MinIO (via Docker Compose)

    (Optional) AWS CLI or mc (MinIO Client)


---

## ðŸ—ºï¸ **Diagram architektury â€” jak zrobiÄ‡ najproÅ›ciej**

**MoÅ¼esz zrobiÄ‡ taki ukÅ‚ad:**

+-----------+ +-------------+ +----------------+
| MinIO | --[CSV]--> | PySpark | --[Parquet]-> MinIO |
| (S3 API) | | ETL | | (S3 API) |
+-----------+ +-------------+ +----------------+
^ ^
| |
[input/] [output/]
