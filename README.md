# Projects# PySpark ETL demo: CSV → Parquet na S3/MinIO

## Opis projektu

Ten projekt demonstruje prosty pipeline ETL oparty o PySpark i MinIO (symulację AWS S3).  
Celem jest pobranie pliku CSV z „chmury” (MinIO), przetworzenie (czyszczenie i agregacja) w PySpark,  
a następnie zapisanie efektu do formatu Parquet w MinIO.

---

## Krok po kroku: Jak uruchomić lokalnie?

### 1. Uruchom MinIO (Docker Compose)
```bash
docker-compose up -d
2. Stwórz bucket demo w MinIO (http://localhost:9001)

    Stwórz foldery: input i output.

3. Wrzuć przykładowy plik CSV do demo/input/ (np. sample.csv).
4. Odpal pipeline PySpark:

spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.367 src/etl_pipeline.py

5. Wynik pojawi się jako plik Parquet w demo/output/.
Co zobaczysz w MinIO po uruchomieniu?

    demo/input/sample.csv — surowe dane wejściowe (CSV)

    demo/output/demo.parquet — dane wyjściowe w formacie Parquet (po przetworzeniu przez Spark)

Wymagania

    Docker + Docker Compose

    PySpark (>=3.3)

    Python 3.8+

    MinIO (uruchamiany przez Docker Compose)

    (Opcjonalnie) AWS CLI lub mc (MinIO Client)

PySpark ETL demo: CSV → Parquet on S3/MinIO
Project description

This project demonstrates a simple ETL pipeline using PySpark and MinIO (S3-compatible storage).
The goal is to fetch a CSV file from the "cloud" (MinIO), process it in PySpark (cleaning and aggregation),
and save the result as a Parquet file back to MinIO.
Step by step: How to run locally?
1. Start MinIO (Docker Compose)

docker-compose up -d

2. Create a demo bucket in MinIO (http://localhost:9001)

    Create input and output folders.

3. Upload a sample CSV file to demo/input/ (e.g., sample.csv).
4. Run the PySpark pipeline:

spark-submit --packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.367 src/etl_pipeline.py

5. The result will appear as a Parquet file in demo/output/.
What will you see in MinIO after running the pipeline?

    demo/input/sample.csv — raw input data (CSV)

    demo/output/demo.parquet — processed data (Parquet, generated by Spark)

Requirements

    Docker + Docker Compose

    PySpark (>=3.3)

    Python 3.8+

    MinIO (via Docker Compose)

    (Optional) AWS CLI or mc (MinIO Client)


---

## 🗺️ **Diagram architektury — jak zrobić najprościej**

**Możesz zrobić taki układ:**

+-----------+ +-------------+ +----------------+
| MinIO | --[CSV]--> | PySpark | --[Parquet]-> MinIO |
| (S3 API) | | ETL | | (S3 API) |
+-----------+ +-------------+ +----------------+
^ ^
| |
[input/] [output/]
